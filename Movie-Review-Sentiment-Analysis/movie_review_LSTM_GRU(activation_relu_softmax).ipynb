{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['movie-review-sentiment-analysis-kernels-only', 'glove-global-vectors-for-word-representation', 'fasttext-crawl-300d-2m']\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import datetime\n",
    "import lightgbm as lgb\n",
    "from scipy import stats\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "pd.set_option('max_colwidth',400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "570249da8b1fe9380b299676e772ef2ff5896306"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/movie-review-sentiment-analysis-kernels-only/train.tsv',delimiter='\\t',encoding='utf-8')\n",
    "test = pd.read_csv('../input/movie-review-sentiment-analysis-kernels-only/test.tsv',delimiter='\\t',encoding='utf-8')\n",
    "sub = pd.read_csv('../input/movie-review-sentiment-analysis-kernels-only/sampleSubmission.csv', sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "dc5ad9dc3d0624212a0f18a7af5ae0a56c88cb4e"
   },
   "outputs": [],
   "source": [
    "tokenizer = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "214cdd6ab00b4c7d4f9b9150169e22e24de71a80"
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2), tokenizer=tokenizer.tokenize)\n",
    "full_text = list(train['Phrase'].values) + list(test['Phrase'].values)\n",
    "vectorizer.fit(full_text)\n",
    "train_vectorized = vectorizer.transform(train['Phrase'])\n",
    "test_vectorized = vectorizer.transform(test['Phrase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "67da69ed129aff55b5a0fac1e3abd0f2f7f88183"
   },
   "outputs": [],
   "source": [
    "y = train['Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "b591deeca045d80c312f9e229c183307ace23242"
   },
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()\n",
    "ovr = OneVsRestClassifier(logreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "d64360fc92dc880208bd2c09b0b9e3cbe1ef47c4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.82 s, sys: 16 ms, total: 6.83 s\n",
      "Wall time: 6.83 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False),\n",
       "          n_jobs=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "ovr.fit(train_vectorized, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "719f0ebadfdd451c6f0008eb682ce16be87497c2"
   },
   "source": [
    "#### n_jobs : int or None, optional (default=None)\n",
    "The number of CPUs to use to do the computation. None means 1 unless in a joblib.parallel_backend context. -1 means using all processors. See Glossary for more details.\n",
    "#### cv : int, cross-validation generator or an iterable, optional Determines the cross-validation splitting strategy. Possible inputs for cv are:\n",
    "\n",
    "None, to use the default 3-fold cross validation,\n",
    "integer, to specify the number of folds in a (Stratified)KFold,\n",
    "CV splitter,\n",
    "An iterable yielding (train, test) splits as arrays of indices.\n",
    "For integer/None inputs, if the estimator is a classifier and y is either binary or multiclass, StratifiedKFold is used. In all other cases, KFold is used.\n",
    "\n",
    "Refer User Guide for the various cross-validation strategies that can be used her"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "db46b73a716f906b28b81ebd66bbb0a5354905ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation mean accuracy 56.55%, std 0.07.\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(ovr, train_vectorized, y, scoring='accuracy', n_jobs=-1, cv=3)\n",
    "print('Cross-validation mean accuracy {0:.2f}%, std {1:.2f}.'.format(np.mean(scores) * 100, np.std(scores) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "6ea21c878d7d12c51f47f33057f470509b7ba736"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation mean accuracy 56.51%, std 0.68.\n",
      "CPU times: user 72 ms, sys: 28 ms, total: 100 ms\n",
      "Wall time: 20.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "svc = LinearSVC(dual=False)\n",
    "scores = cross_val_score(svc, train_vectorized, y, scoring='accuracy', n_jobs=-1, cv=3)\n",
    "print('Cross-validation mean accuracy {0:.2f}%, std {1:.2f}.'.format(np.mean(scores) * 100, np.std(scores) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "455307e6524e5a5da8bc71dfe4e0180c3720e8b7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "ovr.fit(train_vectorized, y);\n",
    "svc.fit(train_vectorized, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "036388eea6f8418e6b8336462182be371c5cb2d6"
   },
   "source": [
    "## Deep learning\n",
    "And now let's try DL. DL should work better for text classification with multiple layers. I use an architecture similar to those which were used in toxic competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "f0acb9c77834ccfcac81eb5b6b840a89a7e8fecd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D, GRU, CuDNNGRU, CuDNNLSTM, BatchNormalization\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D, Add, Flatten\n",
    "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\n",
    "from keras.models import Model, load_model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n",
    "from keras import backend as K\n",
    "from keras.engine import InputSpec, Layer\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, Callback, EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b14708b1ea0fc549c552e07ae4643c3f31e70403"
   },
   "source": [
    "#### t.fit_texts()\n",
    "The tokenizer provides:\n",
    "- word counts\n",
    "- word documensts\n",
    "- word index\n",
    "- document count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c6c5204cadc8dcec14bd67c0573ffe7bccc4b275"
   },
   "source": [
    "#### lower: boolean. Whether to convert the texts to lowercase.\n",
    "#### filters: a string where each element is a character that will be filtered from the texts. The default is all punctuation, plus tabs and line breaks, minus the ' character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "168928f78760dd07bf0146868675f99f2cc417c3"
   },
   "outputs": [],
   "source": [
    "tk = Tokenizer(lower = True, filters='')\n",
    "tk.fit_on_texts(full_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3d1cd746fdf18635ae1203cbc07262f75dbcef52"
   },
   "source": [
    "#### texts_to_sequences()\n",
    "only top \"num_words\" most frequent words will be taken into account. Only word known by the tokenizer will  be taken into account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "379a9edeaf2067672b86cdec12e95e78c490a10c"
   },
   "outputs": [],
   "source": [
    "train_tokenized = tk.texts_to_sequences(train['Phrase'])\n",
    "test_tokenized = tk.texts_to_sequences(test['Phrase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "a1816a465379621b7345088b5f1145103f0bfb88"
   },
   "outputs": [],
   "source": [
    "max_len = 50\n",
    "X_train = pad_sequences(train_tokenized, maxlen = max_len)\n",
    "X_test = pad_sequences(test_tokenized, maxlen = max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "5bfc9fe49271e8ad61571af7a2f2181e173eafe0"
   },
   "outputs": [],
   "source": [
    "embedding_path = \"../input/glove-global-vectors-for-word-representation/glove.6B.100d.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "d5d2e13eba10a1c156766e96a79638a25b1736a6"
   },
   "outputs": [],
   "source": [
    "embed_size = 100\n",
    "max_features = 30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "4379dbf4df91aba284554427fd138050a597e59e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.038194   -0.24487001  0.72812003 -0.39961001  0.083172    0.043953\n",
      " -0.39140999  0.3344     -0.57545     0.087459    0.28786999 -0.06731\n",
      "  0.30906001 -0.26383999 -0.13231    -0.20757     0.33395001 -0.33848\n",
      " -0.31742999 -0.48335999  0.1464     -0.37303999  0.34577     0.052041\n",
      "  0.44946    -0.46970999  0.02628    -0.54154998 -0.15518001 -0.14106999\n",
      " -0.039722    0.28277001  0.14393     0.23464    -0.31020999  0.086173\n",
      "  0.20397     0.52623999  0.17163999 -0.082378   -0.71787    -0.41531\n",
      "  0.20334999 -0.12763     0.41367     0.55186999  0.57907999 -0.33476999\n",
      " -0.36559001 -0.54856998 -0.062892    0.26583999  0.30204999  0.99774998\n",
      " -0.80480999 -3.0243001   0.01254    -0.36941999  2.21670008  0.72201002\n",
      " -0.24978     0.92136002  0.034514    0.46744999  1.10790002 -0.19358\n",
      " -0.074575    0.23353    -0.052062   -0.22044     0.057162   -0.15806\n",
      " -0.30798    -0.41624999  0.37972     0.15006    -0.53211999 -0.20550001\n",
      " -1.25259995  0.071624    0.70564997  0.49744001 -0.42063001  0.26148\n",
      " -1.53799999 -0.30223    -0.073438   -0.28312001  0.37103999 -0.25217\n",
      "  0.016215   -0.017099   -0.38984001  0.87423998 -0.72569001 -0.51058\n",
      " -0.52028    -0.1459      0.82779998  0.27061999]\n"
     ]
    }
   ],
   "source": [
    "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "embedding_index = dict(get_coefs(*o.strip().split(\" \")) for o in open(embedding_path))\n",
    "\n",
    "word_index = tk.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words + 1, embed_size))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "print(embedding_matrix[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "89862f4f8fa8fc58aa7ddae77900ced50dcac2ca"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder(sparse=False)\n",
    "y_ohe = ohe.fit_transform(y.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "dee9848425a9832d7520b7597f00d821073e451b"
   },
   "source": [
    "#### ModelCheckpoint:\n",
    "Save the model after every epoch.\n",
    "#### lr: float >= 0. Learning rate.\n",
    "#### decay: float >= 0. Learning rate decay over each update.\n",
    "#### units: Positive integer, dimensionality of the output space.\n",
    "#### filters/conv_size: Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "6a238e6151508d0c2dc25ac109dae6dace0f163b"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "12a3390186c9fd9672352d390ec3538a5b317cad"
   },
   "outputs": [],
   "source": [
    "def build_model3(lr=0.0, lr_d=0.0, units=0, spatial_dr=0.0, kernel_size1=3, kernel_size2=2, dense_units=128, dr=0.1, conv_size=32):\n",
    "    file_path = \"best_model.hdf5\"\n",
    "    check_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1,\n",
    "                                  save_best_only = True, mode = \"min\")\n",
    "    early_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 3)\n",
    "    \n",
    "    inp = Input(shape = (max_len,))\n",
    "    x = Embedding(19479, embed_size, weights = [embedding_matrix], trainable = False)(inp)\n",
    "    x1 = SpatialDropout1D(spatial_dr)(x)\n",
    "\n",
    "    x_gru = Bidirectional(CuDNNLSTM(units, return_sequences = True))(x1)\n",
    "    x1 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_gru)\n",
    "    avg_pool1_gru = GlobalAveragePooling1D()(x1)\n",
    "    max_pool1_gru = GlobalMaxPooling1D()(x1)\n",
    "    \n",
    "    x3 = Conv1D(conv_size, kernel_size=kernel_size2, padding='valid', kernel_initializer='he_uniform')(x_gru)\n",
    "    avg_pool3_gru = GlobalAveragePooling1D()(x3)\n",
    "    max_pool3_gru = GlobalMaxPooling1D()(x3)\n",
    "    \n",
    "    x_lstm = Bidirectional(CuDNNGRU(units, return_sequences = True))(x1)\n",
    "    x1 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n",
    "    avg_pool1_lstm = GlobalAveragePooling1D()(x1)\n",
    "    max_pool1_lstm = GlobalMaxPooling1D()(x1)\n",
    "    \n",
    "    x3 = Conv1D(conv_size, kernel_size=kernel_size2, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n",
    "    avg_pool3_lstm = GlobalAveragePooling1D()(x3)\n",
    "    max_pool3_lstm = GlobalMaxPooling1D()(x3)\n",
    "    \n",
    "    \n",
    "    x = concatenate([avg_pool1_gru, max_pool1_gru, avg_pool3_gru, max_pool3_gru,\n",
    "                    avg_pool1_lstm, max_pool1_lstm, avg_pool3_lstm, max_pool3_lstm])\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dr)(Dense(dense_units, activation='relu') (x))\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dr)(Dense(int(dense_units / 2), activation='relu') (x))\n",
    "    x = Dense(5, activation = \"softmax\")(x)\n",
    "    model = Model(inputs = inp, outputs = x)\n",
    "    model.compile(loss = \"binary_crossentropy\", optimizer = Adam(lr = lr, decay = lr_d), metrics = [\"accuracy\"])\n",
    "    history = model.fit(X_train, y_ohe, batch_size = 128, epochs = 20, validation_split=0.1, \n",
    "                        verbose = 1, callbacks = [check_point, early_stop])\n",
    "    model = load_model(file_path)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "02c78dd46721aa83cae206579b9370d95cc8aba8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 140454 samples, validate on 15606 samples\n",
      "Epoch 1/20\n",
      "140454/140454 [==============================] - 34s 243us/step - loss: 0.3587 - acc: 0.8435 - val_loss: 0.3404 - val_acc: 0.8448\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.34044, saving model to best_model.hdf5\n",
      "Epoch 2/20\n",
      "140454/140454 [==============================] - 30s 214us/step - loss: 0.3326 - acc: 0.8503 - val_loss: 0.3334 - val_acc: 0.8472\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.34044 to 0.33342, saving model to best_model.hdf5\n",
      "Epoch 3/20\n",
      "140454/140454 [==============================] - 30s 213us/step - loss: 0.3226 - acc: 0.8540 - val_loss: 0.3252 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.33342 to 0.32520, saving model to best_model.hdf5\n",
      "Epoch 4/20\n",
      "140454/140454 [==============================] - 30s 214us/step - loss: 0.3157 - acc: 0.8564 - val_loss: 0.3243 - val_acc: 0.8506\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.32520 to 0.32433, saving model to best_model.hdf5\n",
      "Epoch 5/20\n",
      "140454/140454 [==============================] - 30s 213us/step - loss: 0.3097 - acc: 0.8590 - val_loss: 0.3232 - val_acc: 0.8519\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.32433 to 0.32323, saving model to best_model.hdf5\n",
      "Epoch 6/20\n",
      "140454/140454 [==============================] - 30s 212us/step - loss: 0.3056 - acc: 0.8602 - val_loss: 0.3228 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.32323 to 0.32279, saving model to best_model.hdf5\n",
      "Epoch 7/20\n",
      "140454/140454 [==============================] - 30s 212us/step - loss: 0.3019 - acc: 0.8618 - val_loss: 0.3169 - val_acc: 0.8547\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.32279 to 0.31687, saving model to best_model.hdf5\n",
      "Epoch 8/20\n",
      "140454/140454 [==============================] - 30s 214us/step - loss: 0.2985 - acc: 0.8638 - val_loss: 0.3225 - val_acc: 0.8500\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.31687\n",
      "Epoch 9/20\n",
      "140454/140454 [==============================] - 30s 214us/step - loss: 0.2964 - acc: 0.8648 - val_loss: 0.3245 - val_acc: 0.8539\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.31687\n",
      "Epoch 10/20\n",
      "140454/140454 [==============================] - 30s 214us/step - loss: 0.2932 - acc: 0.8662 - val_loss: 0.3166 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.31687 to 0.31661, saving model to best_model.hdf5\n",
      "Epoch 11/20\n",
      "140454/140454 [==============================] - 30s 214us/step - loss: 0.2915 - acc: 0.8669 - val_loss: 0.3244 - val_acc: 0.8541\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.31661\n",
      "Epoch 12/20\n",
      "104576/140454 [=====================>........] - ETA: 7s - loss: 0.2882 - acc: 0.8681"
     ]
    }
   ],
   "source": [
    "model8 = build_model3(lr = 1e-3, lr_d = 1e-10, units = 64, spatial_dr = 0.3, kernel_size1=3, kernel_size2=2, dense_units=32, dr=0.1, conv_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "d50fd9ee6a034b4f07d7885596d324ca1a450a9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 140454 samples, validate on 15606 samples\n",
      "Epoch 1/20\n",
      "140454/140454 [==============================] - 33s 236us/step - loss: 0.3583 - acc: 0.8437 - val_loss: 0.3412 - val_acc: 0.8435\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.34123, saving model to best_model.hdf5\n",
      "Epoch 2/20\n",
      "140454/140454 [==============================] - 30s 216us/step - loss: 0.3335 - acc: 0.8502 - val_loss: 0.3359 - val_acc: 0.8476\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.34123 to 0.33595, saving model to best_model.hdf5\n",
      "Epoch 3/20\n",
      "140454/140454 [==============================] - 30s 215us/step - loss: 0.3243 - acc: 0.8531 - val_loss: 0.3286 - val_acc: 0.8495\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.33595 to 0.32857, saving model to best_model.hdf5\n",
      "Epoch 4/20\n",
      "140454/140454 [==============================] - 30s 215us/step - loss: 0.3171 - acc: 0.8556 - val_loss: 0.3242 - val_acc: 0.8508\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.32857 to 0.32423, saving model to best_model.hdf5\n",
      "Epoch 5/20\n",
      "140454/140454 [==============================] - 30s 215us/step - loss: 0.3111 - acc: 0.8579 - val_loss: 0.3278 - val_acc: 0.8486\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.32423\n",
      "Epoch 6/20\n",
      "140454/140454 [==============================] - 30s 215us/step - loss: 0.3066 - acc: 0.8598 - val_loss: 0.3184 - val_acc: 0.8533\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.32423 to 0.31835, saving model to best_model.hdf5\n",
      "Epoch 7/20\n",
      "140454/140454 [==============================] - 30s 216us/step - loss: 0.3029 - acc: 0.8616 - val_loss: 0.3203 - val_acc: 0.8527\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.31835\n",
      "Epoch 8/20\n",
      "140454/140454 [==============================] - 30s 214us/step - loss: 0.2997 - acc: 0.8628 - val_loss: 0.3161 - val_acc: 0.8542\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.31835 to 0.31614, saving model to best_model.hdf5\n",
      "Epoch 9/20\n",
      "140454/140454 [==============================] - 30s 215us/step - loss: 0.2966 - acc: 0.8640 - val_loss: 0.3211 - val_acc: 0.8524\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.31614\n",
      "Epoch 10/20\n",
      "140454/140454 [==============================] - 30s 216us/step - loss: 0.2943 - acc: 0.8659 - val_loss: 0.3225 - val_acc: 0.8517\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.31614\n",
      "Epoch 11/20\n",
      "140454/140454 [==============================] - 30s 215us/step - loss: 0.2923 - acc: 0.8664 - val_loss: 0.3169 - val_acc: 0.8531\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.31614\n"
     ]
    }
   ],
   "source": [
    "model9 = build_model3(lr = 1e-3, lr_d = 1e-10, units = 64, spatial_dr = 0.3, kernel_size1=3, kernel_size2=2, dense_units=32, dr=0.1, conv_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "99e36215a027dac95219225e03740a2f3a1d0360"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66292/66292 [==============================] - 2s 27us/step\n",
      "66292/66292 [==============================] - 2s 25us/step\n"
     ]
    }
   ],
   "source": [
    "pred8 = model8.predict(X_test, batch_size = 1024, verbose = 1)\n",
    "pred = pred8\n",
    "pred9 = model9.predict(X_test, batch_size = 1024, verbose = 1)\n",
    "pred += pred9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "f8065f1cd3c4400620964f819c0e75efb279d8cf"
   },
   "outputs": [],
   "source": [
    "predictions = np.round(np.argmax(pred, axis=1)).astype(int)\n",
    "sub['Sentiment'] = predictions\n",
    "sub.to_csv(\"blend.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
