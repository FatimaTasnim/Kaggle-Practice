{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['movie-review-sentiment-analysis-kernels-only', 'glove-global-vectors-for-word-representation', 'fasttext-crawl-300d-2m']\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import datetime\n",
    "import lightgbm as lgb\n",
    "from scipy import stats\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "pd.set_option('max_colwidth',400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "570249da8b1fe9380b299676e772ef2ff5896306"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/movie-review-sentiment-analysis-kernels-only/train.tsv',delimiter='\\t',encoding='utf-8')\n",
    "test = pd.read_csv('../input/movie-review-sentiment-analysis-kernels-only/test.tsv',delimiter='\\t',encoding='utf-8')\n",
    "sub = pd.read_csv('../input/movie-review-sentiment-analysis-kernels-only/sampleSubmission.csv', sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "c769bc49b7834ab4e3e0234ad74d9f6868b5203e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage that what is good for the goose is also good for the gander , some of which occasionally amuses but none of which amounts to much of a story .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage that what is good for the goose</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>of escapades demonstrating the adage that what is good for the goose</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>of</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>escapades demonstrating the adage that what is good for the goose</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>escapades</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>demonstrating the adage that what is good for the goose</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId    ...      Sentiment\n",
       "0         1    ...              1\n",
       "1         2    ...              2\n",
       "2         3    ...              2\n",
       "3         4    ...              2\n",
       "4         5    ...              2\n",
       "5         6    ...              2\n",
       "6         7    ...              2\n",
       "7         8    ...              2\n",
       "8         9    ...              2\n",
       "9        10    ...              2\n",
       "\n",
       "[10 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "79c4d60a75b706e4960f1d3ceb97aad4c62d3622"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>This quiet , introspective and entertaining independent is worth seeking .</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>65</td>\n",
       "      <td>2</td>\n",
       "      <td>This quiet , introspective and entertaining independent</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>66</td>\n",
       "      <td>2</td>\n",
       "      <td>This</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>67</td>\n",
       "      <td>2</td>\n",
       "      <td>quiet , introspective and entertaining independent</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>68</td>\n",
       "      <td>2</td>\n",
       "      <td>quiet , introspective and entertaining</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>69</td>\n",
       "      <td>2</td>\n",
       "      <td>quiet</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>70</td>\n",
       "      <td>2</td>\n",
       "      <td>, introspective and entertaining</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>71</td>\n",
       "      <td>2</td>\n",
       "      <td>introspective and entertaining</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>72</td>\n",
       "      <td>2</td>\n",
       "      <td>introspective and</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>73</td>\n",
       "      <td>2</td>\n",
       "      <td>introspective</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>74</td>\n",
       "      <td>2</td>\n",
       "      <td>and</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>75</td>\n",
       "      <td>2</td>\n",
       "      <td>entertaining</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>76</td>\n",
       "      <td>2</td>\n",
       "      <td>independent</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>77</td>\n",
       "      <td>2</td>\n",
       "      <td>is worth seeking .</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>78</td>\n",
       "      <td>2</td>\n",
       "      <td>is worth seeking</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>79</td>\n",
       "      <td>2</td>\n",
       "      <td>is worth</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>80</td>\n",
       "      <td>2</td>\n",
       "      <td>worth</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>81</td>\n",
       "      <td>2</td>\n",
       "      <td>seeking</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    PhraseId    ...      Sentiment\n",
       "63        64    ...              4\n",
       "64        65    ...              3\n",
       "65        66    ...              2\n",
       "66        67    ...              4\n",
       "67        68    ...              3\n",
       "68        69    ...              2\n",
       "69        70    ...              3\n",
       "70        71    ...              3\n",
       "71        72    ...              3\n",
       "72        73    ...              2\n",
       "73        74    ...              2\n",
       "74        75    ...              4\n",
       "75        76    ...              2\n",
       "76        77    ...              3\n",
       "77        78    ...              4\n",
       "78        79    ...              2\n",
       "79        80    ...              2\n",
       "80        81    ...              2\n",
       "\n",
       "[18 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.loc[train.SentenceId == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "db6a953bd7bcf6f30f5b12aa393bbc51ef49b03f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average count of phrases per sentence in train is 18.\n",
      "Average count of phrases per sentence in test is 20.\n"
     ]
    }
   ],
   "source": [
    "print('Average count of phrases per sentence in train is {0:.0f}.'.format(train.groupby('SentenceId')['Phrase'].count().mean()))\n",
    "print('Average count of phrases per sentence in test is {0:.0f}.'.format(test.groupby('SentenceId')['Phrase'].count().mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "fecf16d2f10ada19ebbf9e3b51d2bcd3661ec0d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of phrases in train: 156060. Number of sentences in train: 8529.\n",
      "Number of phrases in test: 66292. Number of sentences in test: 3310.\n"
     ]
    }
   ],
   "source": [
    "print('Number of phrases in train: {}. Number of sentences in train: {}.'.format(train.shape[0], len(train.SentenceId.unique())))\n",
    "print('Number of phrases in test: {}. Number of sentences in test: {}.'.format(test.shape[0], len(test.SentenceId.unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "5ef36b96b27b405116981e0d741a6a4a09898928"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average word length of phrases in train is 7.\n",
      "Average word length of phrases in test is 7.\n"
     ]
    }
   ],
   "source": [
    "print('Average word length of phrases in train is {0:.0f}.'.format(np.mean(train['Phrase'].apply(lambda x: len(x.split())))))\n",
    "print('Average word length of phrases in test is {0:.0f}.'.format(np.mean(test['Phrase'].apply(lambda x: len(x.split())))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "5b356b7f75fd0cd79ecd03103d563aee97c67ebe"
   },
   "outputs": [],
   "source": [
    "text = ' '.join(train.loc[train.Sentiment == 4, 'Phrase'].values)\n",
    "text_trigrams = [i for i in ngrams(text.split(), 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "23a0b7a9d6f8460cae333a4dd217561af0d744c1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('one', 'of', 'the'), 199),\n",
       " (('of', 'the', 'year'), 103),\n",
       " (('.', 'is', 'a'), 87),\n",
       " (('of', 'the', 'best'), 80),\n",
       " (('of', 'the', 'most'), 70),\n",
       " (('is', 'one', 'of'), 50),\n",
       " (('One', 'of', 'the'), 43),\n",
       " ((',', 'and', 'the'), 40),\n",
       " (('the', 'year', \"'s\"), 38),\n",
       " (('It', \"'s\", 'a'), 38),\n",
       " (('it', \"'s\", 'a'), 37),\n",
       " (('.', \"'s\", 'a'), 37),\n",
       " (('a', 'movie', 'that'), 35),\n",
       " (('the', 'edge', 'of'), 34),\n",
       " (('the', 'kind', 'of'), 33),\n",
       " (('of', 'your', 'seat'), 33),\n",
       " (('the', 'film', 'is'), 31),\n",
       " ((',', 'this', 'is'), 31),\n",
       " (('the', 'film', \"'s\"), 31),\n",
       " ((',', 'the', 'film'), 30),\n",
       " (('film', 'that', 'is'), 30),\n",
       " (('as', 'one', 'of'), 30),\n",
       " (('edge', 'of', 'your'), 29),\n",
       " ((',', 'it', \"'s\"), 27),\n",
       " (('a', 'film', 'that'), 27),\n",
       " (('as', 'well', 'as'), 27),\n",
       " ((',', 'funny', ','), 25),\n",
       " ((',', 'but', 'it'), 23),\n",
       " (('films', 'of', 'the'), 23),\n",
       " (('some', 'of', 'the'), 23)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(text_trigrams).most_common(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "626abc5cf0bc04bdcb04a8d5c2c2abd8872dcf40"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((',', 'funny', ','), 33),\n",
       " (('one', 'year', \"'s\"), 28),\n",
       " (('year', \"'s\", 'best'), 26),\n",
       " (('movies', 'ever', 'made'), 19),\n",
       " ((',', 'solid', 'cast'), 19),\n",
       " (('solid', 'cast', ','), 18),\n",
       " ((\"'ve\", 'ever', 'seen'), 16),\n",
       " (('.', 'It', \"'s\"), 16),\n",
       " ((',', 'making', 'one'), 15),\n",
       " (('best', 'films', 'year'), 15),\n",
       " ((',', 'touching', ','), 15),\n",
       " (('exquisite', 'acting', ','), 15),\n",
       " (('acting', ',', 'inventive'), 14),\n",
       " ((',', 'inventive', 'screenplay'), 14),\n",
       " (('jaw-dropping', 'action', 'sequences'), 14),\n",
       " (('good', 'acting', ','), 14),\n",
       " ((\"'s\", 'best', 'films'), 14),\n",
       " (('I', \"'ve\", 'seen'), 14),\n",
       " (('funny', ',', 'even'), 14),\n",
       " (('best', 'war', 'movies'), 13),\n",
       " (('purely', 'enjoyable', 'satisfying'), 13),\n",
       " (('funny', ',', 'touching'), 13),\n",
       " ((',', 'smart', ','), 13),\n",
       " (('inventive', 'screenplay', ','), 13),\n",
       " (('funniest', 'jokes', 'movie'), 13),\n",
       " (('action', 'sequences', ','), 13),\n",
       " (('sequences', ',', 'striking'), 13),\n",
       " ((',', 'striking', 'villains'), 13),\n",
       " (('exquisite', 'motion', 'picture'), 13),\n",
       " (('war', 'movies', 'ever'), 12)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = ' '.join(train.loc[train.Sentiment == 4, 'Phrase'].values)\n",
    "text = [i for i in text.split() if i not in stopwords.words('english')]\n",
    "text_trigrams = [i for i in ngrams(text, 3)]\n",
    "Counter(text_trigrams).most_common(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "dc5ad9dc3d0624212a0f18a7af5ae0a56c88cb4e"
   },
   "outputs": [],
   "source": [
    "tokenizer = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "214cdd6ab00b4c7d4f9b9150169e22e24de71a80"
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2), tokenizer=tokenizer.tokenize)\n",
    "full_text = list(train['Phrase'].values) + list(test['Phrase'].values)\n",
    "vectorizer.fit(full_text)\n",
    "train_vectorized = vectorizer.transform(train['Phrase'])\n",
    "test_vectorized = vectorizer.transform(test['Phrase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "67da69ed129aff55b5a0fac1e3abd0f2f7f88183"
   },
   "outputs": [],
   "source": [
    "y = train['Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "b591deeca045d80c312f9e229c183307ace23242"
   },
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()\n",
    "ovr = OneVsRestClassifier(logreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "d64360fc92dc880208bd2c09b0b9e3cbe1ef47c4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.89 s, sys: 16 ms, total: 6.91 s\n",
      "Wall time: 6.91 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False),\n",
       "          n_jobs=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "ovr.fit(train_vectorized, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "719f0ebadfdd451c6f0008eb682ce16be87497c2"
   },
   "source": [
    "#### n_jobs : int or None, optional (default=None)\n",
    "The number of CPUs to use to do the computation. None means 1 unless in a joblib.parallel_backend context. -1 means using all processors. See Glossary for more details.\n",
    "#### cv : int, cross-validation generator or an iterable, optional Determines the cross-validation splitting strategy. Possible inputs for cv are:\n",
    "\n",
    "None, to use the default 3-fold cross validation,\n",
    "integer, to specify the number of folds in a (Stratified)KFold,\n",
    "CV splitter,\n",
    "An iterable yielding (train, test) splits as arrays of indices.\n",
    "For integer/None inputs, if the estimator is a classifier and y is either binary or multiclass, StratifiedKFold is used. In all other cases, KFold is used.\n",
    "\n",
    "Refer User Guide for the various cross-validation strategies that can be used her"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "db46b73a716f906b28b81ebd66bbb0a5354905ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation mean accuracy 56.55%, std 0.07.\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(ovr, train_vectorized, y, scoring='accuracy', n_jobs=-1, cv=3)\n",
    "print('Cross-validation mean accuracy {0:.2f}%, std {1:.2f}.'.format(np.mean(scores) * 100, np.std(scores) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "6ea21c878d7d12c51f47f33057f470509b7ba736"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation mean accuracy 56.51%, std 0.68.\n",
      "CPU times: user 68 ms, sys: 32 ms, total: 100 ms\n",
      "Wall time: 20.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "svc = LinearSVC(dual=False)\n",
    "scores = cross_val_score(svc, train_vectorized, y, scoring='accuracy', n_jobs=-1, cv=3)\n",
    "print('Cross-validation mean accuracy {0:.2f}%, std {1:.2f}.'.format(np.mean(scores) * 100, np.std(scores) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "455307e6524e5a5da8bc71dfe4e0180c3720e8b7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "ovr.fit(train_vectorized, y);\n",
    "svc.fit(train_vectorized, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "036388eea6f8418e6b8336462182be371c5cb2d6"
   },
   "source": [
    "## Deep learning\n",
    "And now let's try DL. DL should work better for text classification with multiple layers. I use an architecture similar to those which were used in toxic competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "f0acb9c77834ccfcac81eb5b6b840a89a7e8fecd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D, GRU, CuDNNGRU, CuDNNLSTM, BatchNormalization\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D, Add, Flatten\n",
    "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\n",
    "from keras.models import Model, load_model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n",
    "from keras import backend as K\n",
    "from keras.engine import InputSpec, Layer\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, Callback, EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b14708b1ea0fc549c552e07ae4643c3f31e70403"
   },
   "source": [
    "#### t.fit_texts()\n",
    "The tokenizer provides:\n",
    "- word counts\n",
    "- word documensts\n",
    "- word index\n",
    "- document count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c6c5204cadc8dcec14bd67c0573ffe7bccc4b275"
   },
   "source": [
    "#### lower: boolean. Whether to convert the texts to lowercase.\n",
    "#### filters: a string where each element is a character that will be filtered from the texts. The default is all punctuation, plus tabs and line breaks, minus the ' character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "168928f78760dd07bf0146868675f99f2cc417c3"
   },
   "outputs": [],
   "source": [
    "tk = Tokenizer(lower = True, filters='')\n",
    "tk.fit_on_texts(full_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3d1cd746fdf18635ae1203cbc07262f75dbcef52"
   },
   "source": [
    "#### texts_to_sequences()\n",
    "only top \"num_words\" most frequent words will be taken into account. Only word known by the tokenizer will  be taken into account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "379a9edeaf2067672b86cdec12e95e78c490a10c"
   },
   "outputs": [],
   "source": [
    "train_tokenized = tk.texts_to_sequences(train['Phrase'])\n",
    "test_tokenized = tk.texts_to_sequences(test['Phrase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "a1816a465379621b7345088b5f1145103f0bfb88"
   },
   "outputs": [],
   "source": [
    "max_len = 50\n",
    "X_train = pad_sequences(train_tokenized, maxlen = max_len)\n",
    "X_test = pad_sequences(test_tokenized, maxlen = max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "5bfc9fe49271e8ad61571af7a2f2181e173eafe0"
   },
   "outputs": [],
   "source": [
    "embedding_path = \"../input/glove-global-vectors-for-word-representation/glove.6B.100d.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "d5d2e13eba10a1c156766e96a79638a25b1736a6"
   },
   "outputs": [],
   "source": [
    "embed_size = 100\n",
    "max_features = 30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_uuid": "4379dbf4df91aba284554427fd138050a597e59e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.038194   -0.24487001  0.72812003 -0.39961001  0.083172    0.043953\n",
      " -0.39140999  0.3344     -0.57545     0.087459    0.28786999 -0.06731\n",
      "  0.30906001 -0.26383999 -0.13231    -0.20757     0.33395001 -0.33848\n",
      " -0.31742999 -0.48335999  0.1464     -0.37303999  0.34577     0.052041\n",
      "  0.44946    -0.46970999  0.02628    -0.54154998 -0.15518001 -0.14106999\n",
      " -0.039722    0.28277001  0.14393     0.23464    -0.31020999  0.086173\n",
      "  0.20397     0.52623999  0.17163999 -0.082378   -0.71787    -0.41531\n",
      "  0.20334999 -0.12763     0.41367     0.55186999  0.57907999 -0.33476999\n",
      " -0.36559001 -0.54856998 -0.062892    0.26583999  0.30204999  0.99774998\n",
      " -0.80480999 -3.0243001   0.01254    -0.36941999  2.21670008  0.72201002\n",
      " -0.24978     0.92136002  0.034514    0.46744999  1.10790002 -0.19358\n",
      " -0.074575    0.23353    -0.052062   -0.22044     0.057162   -0.15806\n",
      " -0.30798    -0.41624999  0.37972     0.15006    -0.53211999 -0.20550001\n",
      " -1.25259995  0.071624    0.70564997  0.49744001 -0.42063001  0.26148\n",
      " -1.53799999 -0.30223    -0.073438   -0.28312001  0.37103999 -0.25217\n",
      "  0.016215   -0.017099   -0.38984001  0.87423998 -0.72569001 -0.51058\n",
      " -0.52028    -0.1459      0.82779998  0.27061999]\n"
     ]
    }
   ],
   "source": [
    "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "embedding_index = dict(get_coefs(*o.strip().split(\" \")) for o in open(embedding_path))\n",
    "\n",
    "word_index = tk.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words + 1, embed_size))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "print(embedding_matrix[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_uuid": "89862f4f8fa8fc58aa7ddae77900ced50dcac2ca"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder(sparse=False)\n",
    "y_ohe = ohe.fit_transform(y.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "dee9848425a9832d7520b7597f00d821073e451b"
   },
   "source": [
    "#### ModelCheckpoint:\n",
    "Save the model after every epoch.\n",
    "#### lr: float >= 0. Learning rate.\n",
    "#### decay: float >= 0. Learning rate decay over each update.\n",
    "#### units: Positive integer, dimensionality of the output space.\n",
    "#### filters/conv_size: Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_uuid": "6a238e6151508d0c2dc25ac109dae6dace0f163b"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_uuid": "12a3390186c9fd9672352d390ec3538a5b317cad"
   },
   "outputs": [],
   "source": [
    "def build_model4(lr=0.0, lr_d=0.0, units=0, spatial_dr=0.0, kernel_size1=3, kernel_size2=2, dense_units=128, dr=0.1, conv_size=32):\n",
    "    file_path = \"best_model.hdf5\"\n",
    "    check_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1,\n",
    "                                  save_best_only = True, mode = \"min\")\n",
    "    early_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 3)\n",
    "    \n",
    "    inp = Input(shape = (max_len,))\n",
    "    x = Embedding(19479, embed_size, weights = [embedding_matrix], trainable = False)(inp)\n",
    "    x1 = SpatialDropout1D(spatial_dr)(x)\n",
    "\n",
    "    x_gru = Bidirectional(CuDNNGRU(units, return_sequences = True))(x1)\n",
    "    x1 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_gru)\n",
    "    avg_pool1_gru = GlobalAveragePooling1D()(x1)\n",
    "    max_pool1_gru = GlobalMaxPooling1D()(x1)\n",
    "    \n",
    "    x3 = Conv1D(conv_size, kernel_size=kernel_size2, padding='valid', kernel_initializer='he_uniform')(x_gru)\n",
    "    avg_pool3_gru = GlobalAveragePooling1D()(x3)\n",
    "    max_pool3_gru = GlobalMaxPooling1D()(x3)\n",
    "    \n",
    "    x_lstm = Bidirectional(CuDNNLSTM(units, return_sequences = True))(x1)\n",
    "    x1 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n",
    "    avg_pool1_lstm = GlobalAveragePooling1D()(x1)\n",
    "    max_pool1_lstm = GlobalMaxPooling1D()(x1)\n",
    "    \n",
    "    x3 = Conv1D(conv_size, kernel_size=kernel_size2, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n",
    "    avg_pool3_lstm = GlobalAveragePooling1D()(x3)\n",
    "    max_pool3_lstm = GlobalMaxPooling1D()(x3)\n",
    "    \n",
    "    \n",
    "    x = concatenate([avg_pool1_gru, max_pool1_gru, avg_pool3_gru, max_pool3_gru,\n",
    "                    avg_pool1_lstm, max_pool1_lstm, avg_pool3_lstm, max_pool3_lstm])\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dr)(Dense(dense_units, activation='relu') (x))\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dr)(Dense(int(dense_units / 2), activation='relu') (x))\n",
    "    x = Dense(5, activation = \"softmax\")(x)\n",
    "    model = Model(inputs = inp, outputs = x)\n",
    "    model.compile(loss = \"binary_crossentropy\", optimizer = Adam(lr = lr, decay = lr_d), metrics = [\"accuracy\"])\n",
    "    history = model.fit(X_train, y_ohe, batch_size = 128, epochs = 20, validation_split=0.1, \n",
    "                        verbose = 1, callbacks = [check_point, early_stop])\n",
    "    model = load_model(file_path)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_uuid": "02c78dd46721aa83cae206579b9370d95cc8aba8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 140454 samples, validate on 15606 samples\n",
      "Epoch 1/20\n",
      "140454/140454 [==============================] - 34s 240us/step - loss: 0.3597 - acc: 0.8428 - val_loss: 0.3469 - val_acc: 0.8415\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.34694, saving model to best_model.hdf5\n",
      "Epoch 2/20\n",
      "140454/140454 [==============================] - 30s 215us/step - loss: 0.3338 - acc: 0.8501 - val_loss: 0.3346 - val_acc: 0.8467\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.34694 to 0.33461, saving model to best_model.hdf5\n",
      "Epoch 3/20\n",
      "140454/140454 [==============================] - 30s 216us/step - loss: 0.3249 - acc: 0.8526 - val_loss: 0.3255 - val_acc: 0.8496\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.33461 to 0.32552, saving model to best_model.hdf5\n",
      "Epoch 4/20\n",
      "140454/140454 [==============================] - 30s 216us/step - loss: 0.3182 - acc: 0.8555 - val_loss: 0.3310 - val_acc: 0.8468\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.32552\n",
      "Epoch 5/20\n",
      "140454/140454 [==============================] - 30s 215us/step - loss: 0.3122 - acc: 0.8580 - val_loss: 0.3237 - val_acc: 0.8503\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.32552 to 0.32365, saving model to best_model.hdf5\n",
      "Epoch 6/20\n",
      "140454/140454 [==============================] - 30s 216us/step - loss: 0.3070 - acc: 0.8598 - val_loss: 0.3181 - val_acc: 0.8528\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.32365 to 0.31811, saving model to best_model.hdf5\n",
      "Epoch 7/20\n",
      "140454/140454 [==============================] - 30s 214us/step - loss: 0.3037 - acc: 0.8613 - val_loss: 0.3205 - val_acc: 0.8518\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.31811\n",
      "Epoch 8/20\n",
      "140454/140454 [==============================] - 30s 215us/step - loss: 0.3004 - acc: 0.8626 - val_loss: 0.3184 - val_acc: 0.8541\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.31811\n",
      "Epoch 9/20\n",
      "140454/140454 [==============================] - 30s 216us/step - loss: 0.2974 - acc: 0.8641 - val_loss: 0.3188 - val_acc: 0.8541\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.31811\n"
     ]
    }
   ],
   "source": [
    "model8 = build_model4(lr = 1e-3, lr_d = 1e-10, units = 64, spatial_dr = 0.3, kernel_size1=3, kernel_size2=2, dense_units=32, dr=0.1, conv_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_uuid": "d50fd9ee6a034b4f07d7885596d324ca1a450a9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 140454 samples, validate on 15606 samples\n",
      "Epoch 1/20\n",
      "140454/140454 [==============================] - 33s 238us/step - loss: 0.3609 - acc: 0.8438 - val_loss: 0.3424 - val_acc: 0.8439\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.34237, saving model to best_model.hdf5\n",
      "Epoch 2/20\n",
      "140454/140454 [==============================] - 30s 217us/step - loss: 0.3341 - acc: 0.8500 - val_loss: 0.3342 - val_acc: 0.8469\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.34237 to 0.33418, saving model to best_model.hdf5\n",
      "Epoch 3/20\n",
      "140454/140454 [==============================] - 31s 218us/step - loss: 0.3256 - acc: 0.8529 - val_loss: 0.3325 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.33418 to 0.33250, saving model to best_model.hdf5\n",
      "Epoch 4/20\n",
      "140454/140454 [==============================] - 31s 218us/step - loss: 0.3187 - acc: 0.8548 - val_loss: 0.3312 - val_acc: 0.8467\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.33250 to 0.33123, saving model to best_model.hdf5\n",
      "Epoch 5/20\n",
      "140454/140454 [==============================] - 31s 218us/step - loss: 0.3130 - acc: 0.8578 - val_loss: 0.3244 - val_acc: 0.8499\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.33123 to 0.32443, saving model to best_model.hdf5\n",
      "Epoch 6/20\n",
      "140454/140454 [==============================] - 31s 218us/step - loss: 0.3081 - acc: 0.8593 - val_loss: 0.3243 - val_acc: 0.8521\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.32443 to 0.32428, saving model to best_model.hdf5\n",
      "Epoch 7/20\n",
      "140454/140454 [==============================] - 30s 217us/step - loss: 0.3048 - acc: 0.8608 - val_loss: 0.3205 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.32428 to 0.32053, saving model to best_model.hdf5\n",
      "Epoch 8/20\n",
      "140454/140454 [==============================] - 30s 217us/step - loss: 0.3017 - acc: 0.8620 - val_loss: 0.3236 - val_acc: 0.8509\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.32053\n",
      "Epoch 9/20\n",
      "140454/140454 [==============================] - 31s 217us/step - loss: 0.2991 - acc: 0.8634 - val_loss: 0.3201 - val_acc: 0.8518\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.32053 to 0.32013, saving model to best_model.hdf5\n",
      "Epoch 10/20\n",
      "140454/140454 [==============================] - 31s 217us/step - loss: 0.2966 - acc: 0.8644 - val_loss: 0.3263 - val_acc: 0.8517\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.32013\n",
      "Epoch 11/20\n",
      "140454/140454 [==============================] - 31s 218us/step - loss: 0.2945 - acc: 0.8653 - val_loss: 0.3182 - val_acc: 0.8526\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.32013 to 0.31825, saving model to best_model.hdf5\n",
      "Epoch 12/20\n",
      "140454/140454 [==============================] - 31s 217us/step - loss: 0.2929 - acc: 0.8662 - val_loss: 0.3228 - val_acc: 0.8507\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.31825\n",
      "Epoch 13/20\n",
      "140454/140454 [==============================] - 30s 217us/step - loss: 0.2906 - acc: 0.8679 - val_loss: 0.3190 - val_acc: 0.8534\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.31825\n",
      "Epoch 14/20\n",
      "140454/140454 [==============================] - 30s 217us/step - loss: 0.2894 - acc: 0.8677 - val_loss: 0.3187 - val_acc: 0.8529\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.31825\n"
     ]
    }
   ],
   "source": [
    "model9 = build_model4(lr = 1e-3, lr_d = 1e-10, units = 64, spatial_dr = 0.3, kernel_size1=3, kernel_size2=2, dense_units=32, dr=0.1, conv_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "_uuid": "99e36215a027dac95219225e03740a2f3a1d0360"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66292/66292 [==============================] - 2s 27us/step\n",
      "66292/66292 [==============================] - 2s 25us/step\n"
     ]
    }
   ],
   "source": [
    "pred8 = model8.predict(X_test, batch_size = 1024, verbose = 1)\n",
    "pred = pred8\n",
    "pred9 = model9.predict(X_test, batch_size = 1024, verbose = 1)\n",
    "pred += pred9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_uuid": "f8065f1cd3c4400620964f819c0e75efb279d8cf"
   },
   "outputs": [],
   "source": [
    "predictions = np.round(np.argmax(pred, axis=1)).astype(int)\n",
    "sub['Sentiment'] = predictions\n",
    "sub.to_csv(\"blend.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_uuid": "aae285b1643898547605c0294a1645b7d40f20cf"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
