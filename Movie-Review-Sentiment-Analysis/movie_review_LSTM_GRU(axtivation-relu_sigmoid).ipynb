{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['movie-review-sentiment-analysis-kernels-only', 'glove-global-vectors-for-word-representation', 'fasttext-crawl-300d-2m']\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import datetime\n",
    "import lightgbm as lgb\n",
    "from scipy import stats\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "pd.set_option('max_colwidth',400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "570249da8b1fe9380b299676e772ef2ff5896306"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/movie-review-sentiment-analysis-kernels-only/train.tsv',delimiter='\\t',encoding='utf-8')\n",
    "test = pd.read_csv('../input/movie-review-sentiment-analysis-kernels-only/test.tsv',delimiter='\\t',encoding='utf-8')\n",
    "sub = pd.read_csv('../input/movie-review-sentiment-analysis-kernels-only/sampleSubmission.csv', sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "dc5ad9dc3d0624212a0f18a7af5ae0a56c88cb4e"
   },
   "outputs": [],
   "source": [
    "tokenizer = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "214cdd6ab00b4c7d4f9b9150169e22e24de71a80"
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2), tokenizer=tokenizer.tokenize)\n",
    "full_text = list(train['Phrase'].values) + list(test['Phrase'].values)\n",
    "vectorizer.fit(full_text)\n",
    "train_vectorized = vectorizer.transform(train['Phrase'])\n",
    "test_vectorized = vectorizer.transform(test['Phrase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "67da69ed129aff55b5a0fac1e3abd0f2f7f88183"
   },
   "outputs": [],
   "source": [
    "y = train['Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "b591deeca045d80c312f9e229c183307ace23242"
   },
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()\n",
    "ovr = OneVsRestClassifier(logreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "d64360fc92dc880208bd2c09b0b9e3cbe1ef47c4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.88 s, sys: 8 ms, total: 6.88 s\n",
      "Wall time: 6.9 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False),\n",
       "          n_jobs=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "ovr.fit(train_vectorized, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "719f0ebadfdd451c6f0008eb682ce16be87497c2"
   },
   "source": [
    "#### n_jobs : int or None, optional (default=None)\n",
    "The number of CPUs to use to do the computation. None means 1 unless in a joblib.parallel_backend context. -1 means using all processors. See Glossary for more details.\n",
    "#### cv : int, cross-validation generator or an iterable, optional Determines the cross-validation splitting strategy. Possible inputs for cv are:\n",
    "\n",
    "None, to use the default 3-fold cross validation,\n",
    "integer, to specify the number of folds in a (Stratified)KFold,\n",
    "CV splitter,\n",
    "An iterable yielding (train, test) splits as arrays of indices.\n",
    "For integer/None inputs, if the estimator is a classifier and y is either binary or multiclass, StratifiedKFold is used. In all other cases, KFold is used.\n",
    "\n",
    "Refer User Guide for the various cross-validation strategies that can be used her"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "db46b73a716f906b28b81ebd66bbb0a5354905ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation mean accuracy 56.55%, std 0.07.\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(ovr, train_vectorized, y, scoring='accuracy', n_jobs=-1, cv=3)\n",
    "print('Cross-validation mean accuracy {0:.2f}%, std {1:.2f}.'.format(np.mean(scores) * 100, np.std(scores) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "6ea21c878d7d12c51f47f33057f470509b7ba736"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation mean accuracy 56.51%, std 0.68.\n",
      "CPU times: user 60 ms, sys: 28 ms, total: 88 ms\n",
      "Wall time: 20.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "svc = LinearSVC(dual=False)\n",
    "scores = cross_val_score(svc, train_vectorized, y, scoring='accuracy', n_jobs=-1, cv=3)\n",
    "print('Cross-validation mean accuracy {0:.2f}%, std {1:.2f}.'.format(np.mean(scores) * 100, np.std(scores) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "455307e6524e5a5da8bc71dfe4e0180c3720e8b7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "ovr.fit(train_vectorized, y);\n",
    "svc.fit(train_vectorized, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "036388eea6f8418e6b8336462182be371c5cb2d6"
   },
   "source": [
    "## Deep learning\n",
    "And now let's try DL. DL should work better for text classification with multiple layers. I use an architecture similar to those which were used in toxic competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "f0acb9c77834ccfcac81eb5b6b840a89a7e8fecd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D, GRU, CuDNNGRU, CuDNNLSTM, BatchNormalization\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D, Add, Flatten\n",
    "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\n",
    "from keras.models import Model, load_model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n",
    "from keras import backend as K\n",
    "from keras.engine import InputSpec, Layer\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, Callback, EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b14708b1ea0fc549c552e07ae4643c3f31e70403"
   },
   "source": [
    "#### t.fit_texts()\n",
    "The tokenizer provides:\n",
    "- word counts\n",
    "- word documensts\n",
    "- word index\n",
    "- document count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c6c5204cadc8dcec14bd67c0573ffe7bccc4b275"
   },
   "source": [
    "#### lower: boolean. Whether to convert the texts to lowercase.\n",
    "#### filters: a string where each element is a character that will be filtered from the texts. The default is all punctuation, plus tabs and line breaks, minus the ' character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "168928f78760dd07bf0146868675f99f2cc417c3"
   },
   "outputs": [],
   "source": [
    "tk = Tokenizer(lower = True, filters='')\n",
    "tk.fit_on_texts(full_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3d1cd746fdf18635ae1203cbc07262f75dbcef52"
   },
   "source": [
    "#### texts_to_sequences()\n",
    "only top \"num_words\" most frequent words will be taken into account. Only word known by the tokenizer will  be taken into account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "379a9edeaf2067672b86cdec12e95e78c490a10c"
   },
   "outputs": [],
   "source": [
    "train_tokenized = tk.texts_to_sequences(train['Phrase'])\n",
    "test_tokenized = tk.texts_to_sequences(test['Phrase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "a1816a465379621b7345088b5f1145103f0bfb88"
   },
   "outputs": [],
   "source": [
    "max_len = 50\n",
    "X_train = pad_sequences(train_tokenized, maxlen = max_len)\n",
    "X_test = pad_sequences(test_tokenized, maxlen = max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "5bfc9fe49271e8ad61571af7a2f2181e173eafe0"
   },
   "outputs": [],
   "source": [
    "embedding_path = \"../input/glove-global-vectors-for-word-representation/glove.6B.100d.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "d5d2e13eba10a1c156766e96a79638a25b1736a6"
   },
   "outputs": [],
   "source": [
    "embed_size = 100\n",
    "max_features = 30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "4379dbf4df91aba284554427fd138050a597e59e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.038194   -0.24487001  0.72812003 -0.39961001  0.083172    0.043953\n",
      " -0.39140999  0.3344     -0.57545     0.087459    0.28786999 -0.06731\n",
      "  0.30906001 -0.26383999 -0.13231    -0.20757     0.33395001 -0.33848\n",
      " -0.31742999 -0.48335999  0.1464     -0.37303999  0.34577     0.052041\n",
      "  0.44946    -0.46970999  0.02628    -0.54154998 -0.15518001 -0.14106999\n",
      " -0.039722    0.28277001  0.14393     0.23464    -0.31020999  0.086173\n",
      "  0.20397     0.52623999  0.17163999 -0.082378   -0.71787    -0.41531\n",
      "  0.20334999 -0.12763     0.41367     0.55186999  0.57907999 -0.33476999\n",
      " -0.36559001 -0.54856998 -0.062892    0.26583999  0.30204999  0.99774998\n",
      " -0.80480999 -3.0243001   0.01254    -0.36941999  2.21670008  0.72201002\n",
      " -0.24978     0.92136002  0.034514    0.46744999  1.10790002 -0.19358\n",
      " -0.074575    0.23353    -0.052062   -0.22044     0.057162   -0.15806\n",
      " -0.30798    -0.41624999  0.37972     0.15006    -0.53211999 -0.20550001\n",
      " -1.25259995  0.071624    0.70564997  0.49744001 -0.42063001  0.26148\n",
      " -1.53799999 -0.30223    -0.073438   -0.28312001  0.37103999 -0.25217\n",
      "  0.016215   -0.017099   -0.38984001  0.87423998 -0.72569001 -0.51058\n",
      " -0.52028    -0.1459      0.82779998  0.27061999]\n"
     ]
    }
   ],
   "source": [
    "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "embedding_index = dict(get_coefs(*o.strip().split(\" \")) for o in open(embedding_path))\n",
    "\n",
    "word_index = tk.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words + 1, embed_size))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "print(embedding_matrix[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "89862f4f8fa8fc58aa7ddae77900ced50dcac2ca"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder(sparse=False)\n",
    "y_ohe = ohe.fit_transform(y.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "dee9848425a9832d7520b7597f00d821073e451b"
   },
   "source": [
    "#### ModelCheckpoint:\n",
    "Save the model after every epoch.\n",
    "#### lr: float >= 0. Learning rate.\n",
    "#### decay: float >= 0. Learning rate decay over each update.\n",
    "#### units: Positive integer, dimensionality of the output space.\n",
    "#### filters/conv_size: Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "6a238e6151508d0c2dc25ac109dae6dace0f163b"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "12a3390186c9fd9672352d390ec3538a5b317cad"
   },
   "outputs": [],
   "source": [
    "def build_model3(lr=0.0, lr_d=0.0, units=0, spatial_dr=0.0, kernel_size1=3, kernel_size2=2, dense_units=128, dr=0.1, conv_size=32):\n",
    "    file_path = \"best_model.hdf5\"\n",
    "    check_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1,\n",
    "                                  save_best_only = True, mode = \"min\")\n",
    "    early_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 3)\n",
    "    \n",
    "    inp = Input(shape = (max_len,))\n",
    "    x = Embedding(19479, embed_size, weights = [embedding_matrix], trainable = False)(inp)\n",
    "    x1 = SpatialDropout1D(spatial_dr)(x)\n",
    "\n",
    "    x_gru = Bidirectional(CuDNNLSTM(units, return_sequences = True))(x1)\n",
    "    x1 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_gru)\n",
    "    avg_pool1_gru = GlobalAveragePooling1D()(x1)\n",
    "    max_pool1_gru = GlobalMaxPooling1D()(x1)\n",
    "    \n",
    "    x3 = Conv1D(conv_size, kernel_size=kernel_size2, padding='valid', kernel_initializer='he_uniform')(x_gru)\n",
    "    avg_pool3_gru = GlobalAveragePooling1D()(x3)\n",
    "    max_pool3_gru = GlobalMaxPooling1D()(x3)\n",
    "    \n",
    "    x_lstm = Bidirectional(CuDNNGRU(units, return_sequences = True))(x1)\n",
    "    x1 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n",
    "    avg_pool1_lstm = GlobalAveragePooling1D()(x1)\n",
    "    max_pool1_lstm = GlobalMaxPooling1D()(x1)\n",
    "    \n",
    "    x3 = Conv1D(conv_size, kernel_size=kernel_size2, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n",
    "    avg_pool3_lstm = GlobalAveragePooling1D()(x3)\n",
    "    max_pool3_lstm = GlobalMaxPooling1D()(x3)\n",
    "    \n",
    "    \n",
    "    x = concatenate([avg_pool1_gru, max_pool1_gru, avg_pool3_gru, max_pool3_gru,\n",
    "                    avg_pool1_lstm, max_pool1_lstm, avg_pool3_lstm, max_pool3_lstm])\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dr)(Dense(dense_units, activation='relu') (x))\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dr)(Dense(int(dense_units / 2), activation='relu') (x))\n",
    "    x = Dense(5, activation = \"sigmoid\")(x)\n",
    "    model = Model(inputs = inp, outputs = x)\n",
    "    model.compile(loss = \"binary_crossentropy\", optimizer = Adam(lr = lr, decay = lr_d), metrics = [\"accuracy\"])\n",
    "    history = model.fit(X_train, y_ohe, batch_size = 128, epochs = 20, validation_split=0.1, \n",
    "                        verbose = 1, callbacks = [check_point, early_stop])\n",
    "    model = load_model(file_path)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "02c78dd46721aa83cae206579b9370d95cc8aba8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 140454 samples, validate on 15606 samples\n",
      "Epoch 1/20\n",
      "140454/140454 [==============================] - 27s 192us/step - loss: 0.3848 - acc: 0.8290 - val_loss: 0.3479 - val_acc: 0.8419\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.34793, saving model to best_model.hdf5\n",
      "Epoch 2/20\n",
      "140454/140454 [==============================] - 22s 158us/step - loss: 0.3377 - acc: 0.8490 - val_loss: 0.3310 - val_acc: 0.8463\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.34793 to 0.33104, saving model to best_model.hdf5\n",
      "Epoch 3/20\n",
      "140454/140454 [==============================] - 22s 158us/step - loss: 0.3271 - acc: 0.8526 - val_loss: 0.3268 - val_acc: 0.8481\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.33104 to 0.32683, saving model to best_model.hdf5\n",
      "Epoch 4/20\n",
      "140454/140454 [==============================] - 23s 164us/step - loss: 0.3184 - acc: 0.8557 - val_loss: 0.3293 - val_acc: 0.8496\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.32683\n",
      "Epoch 5/20\n",
      "140454/140454 [==============================] - 24s 168us/step - loss: 0.3121 - acc: 0.8581 - val_loss: 0.3229 - val_acc: 0.8522\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.32683 to 0.32286, saving model to best_model.hdf5\n",
      "Epoch 6/20\n",
      "140454/140454 [==============================] - 22s 158us/step - loss: 0.3077 - acc: 0.8595 - val_loss: 0.3272 - val_acc: 0.8467\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.32286\n",
      "Epoch 7/20\n",
      "140454/140454 [==============================] - 22s 158us/step - loss: 0.3033 - acc: 0.8616 - val_loss: 0.3181 - val_acc: 0.8525\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.32286 to 0.31814, saving model to best_model.hdf5\n",
      "Epoch 8/20\n",
      "140454/140454 [==============================] - 23s 166us/step - loss: 0.2999 - acc: 0.8632 - val_loss: 0.3260 - val_acc: 0.8512\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.31814\n",
      "Epoch 9/20\n",
      "140454/140454 [==============================] - 22s 158us/step - loss: 0.2970 - acc: 0.8644 - val_loss: 0.3183 - val_acc: 0.8544\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.31814\n",
      "Epoch 10/20\n",
      "140454/140454 [==============================] - 22s 158us/step - loss: 0.2946 - acc: 0.8655 - val_loss: 0.3168 - val_acc: 0.8543\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.31814 to 0.31683, saving model to best_model.hdf5\n",
      "Epoch 11/20\n",
      "140454/140454 [==============================] - 22s 159us/step - loss: 0.2922 - acc: 0.8661 - val_loss: 0.3160 - val_acc: 0.8543\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.31683 to 0.31598, saving model to best_model.hdf5\n",
      "Epoch 12/20\n",
      "140454/140454 [==============================] - 23s 167us/step - loss: 0.2900 - acc: 0.8673 - val_loss: 0.3207 - val_acc: 0.8547\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.31598\n",
      "Epoch 13/20\n",
      "140454/140454 [==============================] - 22s 158us/step - loss: 0.2883 - acc: 0.8681 - val_loss: 0.3186 - val_acc: 0.8544\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.31598\n",
      "Epoch 14/20\n",
      "140454/140454 [==============================] - 23s 163us/step - loss: 0.2863 - acc: 0.8693 - val_loss: 0.3190 - val_acc: 0.8538\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.31598\n"
     ]
    }
   ],
   "source": [
    "model8 = build_model3(lr = 1e-3, lr_d = 1e-10, units = 64, spatial_dr = 0.3, kernel_size1=3, kernel_size2=2, dense_units=32, dr=0.1, conv_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "d50fd9ee6a034b4f07d7885596d324ca1a450a9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 140454 samples, validate on 15606 samples\n",
      "Epoch 1/20\n",
      "140454/140454 [==============================] - 26s 186us/step - loss: 0.3742 - acc: 0.8353 - val_loss: 0.3495 - val_acc: 0.8411\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.34950, saving model to best_model.hdf5\n",
      "Epoch 2/20\n",
      "140454/140454 [==============================] - 22s 159us/step - loss: 0.3356 - acc: 0.8498 - val_loss: 0.3324 - val_acc: 0.8468\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.34950 to 0.33244, saving model to best_model.hdf5\n",
      "Epoch 3/20\n",
      "140454/140454 [==============================] - 23s 164us/step - loss: 0.3245 - acc: 0.8531 - val_loss: 0.3260 - val_acc: 0.8499\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.33244 to 0.32600, saving model to best_model.hdf5\n",
      "Epoch 4/20\n",
      "140454/140454 [==============================] - 24s 174us/step - loss: 0.3163 - acc: 0.8564 - val_loss: 0.3196 - val_acc: 0.8531\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.32600 to 0.31957, saving model to best_model.hdf5\n",
      "Epoch 5/20\n",
      "140454/140454 [==============================] - 22s 159us/step - loss: 0.3104 - acc: 0.8584 - val_loss: 0.3193 - val_acc: 0.8532\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.31957 to 0.31928, saving model to best_model.hdf5\n",
      "Epoch 6/20\n",
      "140454/140454 [==============================] - 22s 159us/step - loss: 0.3059 - acc: 0.8603 - val_loss: 0.3204 - val_acc: 0.8524\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.31928\n",
      "Epoch 7/20\n",
      "140454/140454 [==============================] - 22s 159us/step - loss: 0.3020 - acc: 0.8623 - val_loss: 0.3217 - val_acc: 0.8525\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.31928\n",
      "Epoch 8/20\n",
      "140454/140454 [==============================] - 23s 167us/step - loss: 0.2986 - acc: 0.8634 - val_loss: 0.3158 - val_acc: 0.8545\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.31928 to 0.31579, saving model to best_model.hdf5\n",
      "Epoch 9/20\n",
      "140454/140454 [==============================] - 22s 159us/step - loss: 0.2957 - acc: 0.8651 - val_loss: 0.3194 - val_acc: 0.8554\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.31579\n",
      "Epoch 10/20\n",
      "140454/140454 [==============================] - 22s 159us/step - loss: 0.2929 - acc: 0.8661 - val_loss: 0.3199 - val_acc: 0.8532\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.31579\n",
      "Epoch 11/20\n",
      "140454/140454 [==============================] - 23s 166us/step - loss: 0.2912 - acc: 0.8669 - val_loss: 0.3165 - val_acc: 0.8542\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.31579\n"
     ]
    }
   ],
   "source": [
    "model9 = build_model3(lr = 1e-3, lr_d = 1e-10, units = 64, spatial_dr = 0.3, kernel_size1=3, kernel_size2=2, dense_units=32, dr=0.1, conv_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "99e36215a027dac95219225e03740a2f3a1d0360"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66292/66292 [==============================] - 2s 25us/step\n",
      "66292/66292 [==============================] - 2s 24us/step\n"
     ]
    }
   ],
   "source": [
    "pred8 = model8.predict(X_test, batch_size = 1024, verbose = 1)\n",
    "pred = pred8\n",
    "pred9 = model9.predict(X_test, batch_size = 1024, verbose = 1)\n",
    "pred += pred9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "f8065f1cd3c4400620964f819c0e75efb279d8cf"
   },
   "outputs": [],
   "source": [
    "predictions = np.round(np.argmax(pred, axis=1)).astype(int)\n",
    "sub['Sentiment'] = predictions\n",
    "sub.to_csv(\"blend.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
